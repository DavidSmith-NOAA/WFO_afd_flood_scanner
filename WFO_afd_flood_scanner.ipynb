{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WFO scanner; Author: Charlie smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import webbrowser\n",
    "import os\n",
    "import datetime\n",
    "import urllib.request\n",
    "import urllib\n",
    "import re\n",
    "import sys\n",
    "import textwrap\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib.pyplot import Axes\n",
    "import geoplot\n",
    "import geoplot.crs as gcrs\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import plotly.graph_objects as go\n",
    "start = datetime.now()\n",
    "date = datetime.today().strftime('%Y_%m_%d')\n",
    "date1 = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfo_list = pd.read_excel('WFO_list.xlsx')\n",
    "\n",
    "wfo_city = wfo_list['City']\n",
    "wfo_city = wfo_city.astype(str)\n",
    "wfo_city= wfo_city.values\n",
    "\n",
    "wfo_state = wfo_list['State']\n",
    "wfo_state = wfo_state.astype(str)\n",
    "wfo_state = wfo_state.values\n",
    "\n",
    "wfo_region = wfo_list['Region']\n",
    "wfo_region = wfo_region.astype(str)\n",
    "wfo_region = wfo_region.values\n",
    "\n",
    "wfo_st_ab = wfo_list['St_ab']\n",
    "wfo_st_ab = wfo_st_ab.astype(str)\n",
    "wfo_st_ab = wfo_st_ab.values\n",
    "\n",
    "wfo_list = wfo_list['CWA']\n",
    "wfo_list = wfo_list.astype(str)\n",
    "wfo_list= wfo_list.values\n",
    "wfo_list = list(wfo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfo_messages = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,len(wfo_list))[0:500]:\n",
    "    try:\n",
    "        wfo_id = wfo_list[i]\n",
    "        \n",
    "        #Open and Read Webpage\n",
    "        url = 'https://forecast.weather.gov/product.php?site='+wfo_id+'&issuedby='+wfo_id+'&product=AFD&format=txt&version=1&glossary=0'\n",
    "        afd = urllib.request.urlopen(url).read()\n",
    "        afd = BeautifulSoup(afd)\n",
    "        x = afd.get_text()\n",
    "        x = str(x)\n",
    "        x = x.replace('\\n', ' ')\n",
    "        x = x.replace('...', '. ')\n",
    "        x = x.replace('...', '. ')\n",
    "\n",
    "        #create empty list\n",
    "        hydro_list = []\n",
    "\n",
    "        #Extract sentenses with \"Flood\" or other related terms\n",
    "\n",
    "        text = x\n",
    "        \n",
    "        # Find the index of the first occurrence of the string \"Discussion\"\n",
    "        start_index = text.find(\"Discussion\")\n",
    "        \n",
    "            \n",
    "\n",
    "        # Select the substring starting from the index of the first occurrence of \"Discussion\"\n",
    "        text = text[start_index:]\n",
    "        \n",
    "        # Regular expression pattern to find the month abbreviations with spaces on either side\n",
    "        month_pattern = r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b'\n",
    "\n",
    "        # Search for the month abbreviations in the text\n",
    "        matches = re.finditer(month_pattern, text)\n",
    "\n",
    "        # Extract the month along with 15 characters before and 8 characters after for each match\n",
    "        for match in matches:\n",
    "            start_index = max(match.start() - 16, 0)\n",
    "            end_index = min(match.end() + 8, len(text))\n",
    "            issuance = text[start_index:end_index]\n",
    "            \n",
    "\n",
    "        # Select the substring starting from the index of the first occurrence of \"Discussion\"\n",
    "        text = text[start_index:]\n",
    "        \n",
    "\n",
    "        words = ['flood']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)\n",
    "                \n",
    "                \n",
    "        words = ['Flood']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)\n",
    "                \n",
    "        words = ['flooding']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)\n",
    "\n",
    "        words = ['debris']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)\n",
    "\n",
    "        words = ['flash']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)\n",
    "\n",
    "        words = ['landslide']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)\n",
    "                \n",
    "        words = ['ice jam']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)\n",
    "                \n",
    "        words = ['river ice']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)\n",
    "\n",
    "        words = ['burn scar']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)\n",
    "                \n",
    "        words = ['burn area']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)\n",
    "                \n",
    "        words = ['mudslide']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)\n",
    "                \n",
    "        words = ['life-threatening']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence)                \n",
    "\n",
    "        words = ['life threatening']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence) \n",
    "                \n",
    "        words = ['snowmelt']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence) \n",
    "                \n",
    "        words = ['SWE']\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if (all(map(lambda word: word in sentence, words))):\n",
    "                hydro_list.append(sentence) \n",
    "                                \n",
    "                \n",
    "\n",
    "        #ave as data frame\n",
    "        hydro_list = pd.DataFrame(np.array(hydro_list))\n",
    "        hydro_list = hydro_list.rename(columns={0: \"Statement\"})\n",
    "        hydro_list = hydro_list.drop_duplicates()\n",
    "        \n",
    "        hydro_list['Issued'] = issuance\n",
    "        hydro_list['WFO'] = wfo_id\n",
    "        hydro_list['City'] = wfo_city[i]\n",
    "        hydro_list['State'] = wfo_state[i]\n",
    "        hydro_list['Region'] = wfo_region [i]\n",
    "        hydro_list['value'] = len(hydro_list.index)\n",
    "        hydro_list['URL'] = url\n",
    "\n",
    "        wfo_messages = wfo_messages.append(hydro_list)\n",
    "        \n",
    "\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfo_messages.to_csv('wfo_messages.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'URL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\dev2\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3802\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3803\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dev2\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dev2\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'URL'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5064\\272316855.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwfo_messages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'URL'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwfo_messages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'URL'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwfo_messages\u001b[0m  \u001b[1;33m=\u001b[0m  \u001b[0mwfo_messages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwfo_messages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwfo_messages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'State'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwfo_messages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwfo_messages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'City'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwfo_messages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwfo_messages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'WFO'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dev2\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3806\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3807\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3808\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dev2\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3803\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3804\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3805\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3806\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'URL'"
     ]
    }
   ],
   "source": [
    "wfo_messages['URL'] = wfo_messages['URL'].astype(str)\n",
    "wfo_messages  =  wfo_messages.reset_index(drop=True)\n",
    "wfo_messages = wfo_messages.set_index('State').reset_index()\n",
    "wfo_messages = wfo_messages.set_index('City').reset_index()\n",
    "wfo_messages = wfo_messages.set_index('WFO').reset_index()\n",
    "wfo_messages = wfo_messages.sort_values(['Region', 'State', 'City'], ascending=[True, True, True])\n",
    "wfo_messages = wfo_messages.reset_index()\n",
    "wfo_messages = wfo_messages.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clickable(val):\n",
    "    # target _blank to open new window\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val, val)\n",
    "\n",
    "wfo_messages.style.format({'URL': make_clickable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfo_messages = wfo_messages.dropna()\n",
    "wfo_messages.to_html('wfo_messages.html', render_links=True)\n",
    "wfo_messages.to_csv('wfo_messages.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfo_loc = gpd.read_file('w_10nv20.shp')\n",
    "wfo_loc.head()\n",
    "wfo_messages = pd.merge(wfo_loc[['WFO', 'geometry']], wfo_messages[['WFO', 'City', 'Issued','Statement', 'URL', 'value']], left_on='WFO', right_on='WFO', how='left')\n",
    "#df_merged = df_merged.dropna(subset=['percent_unemployed', 'geometry']).set_index('LGA_CODE20')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfo_loc = gpd.read_file('w_10nv20.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfo_messages = wfo_messages.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: Display using geopandas\n",
    "fig, ax = plt.subplots(1,1, figsize=(50,50))\n",
    "divider = make_axes_locatable(ax)\n",
    "tmp = wfo_messages.copy()\n",
    "\n",
    "cax = divider.append_axes(\"right\", size=\"3%\", pad=1) #resize the colorbar\n",
    "tmp.plot(column='Statement', ax=ax,cax=cax, legend=False)\n",
    "tmp.geometry.boundary.plot(color='#BBBBBB', ax=ax, linewidth=1) #Add some borders to the geometries\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfo_messages = wfo_messages.to_crs(epsg=4326) # convert the coordinate reference system to lat/long\n",
    "lga_json = wfo_messages.__geo_interface__ #covert to geoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPBOX_ACCESSTOKEN = 'pk.eyJ1IjoiZGF2aWRjc21pdGgiLCJhIjoiY2trYTVxNGdzMGNzbzJ2cjE5ZjZ4NDBvOSJ9.mxK6SMEyWvPz0PGMYyCaIg'\n",
    "\n",
    "zmin = wfo_messages.value.min()\n",
    "zmax = wfo_messages.value.max()\n",
    "\n",
    "\n",
    "# Set the data for the map\n",
    "data = go.Choroplethmapbox(\n",
    "        geojson = lga_json,             #this is your GeoJSON\n",
    "        locations = wfo_messages.index,    #the index of this dataframe should align with the 'id' element in your geojson\n",
    "        z = wfo_messages.value, #sets the color value\n",
    "        text = wfo_messages.WFO+'--'+wfo_messages.City+'--'+wfo_messages.Issued+'<br>'+wfo_messages.Statement,    #sets text for each shape\n",
    "        colorbar=dict(thickness=20, ticklen=0, tickformat='', outlinewidth=.1), #adjusts the format of the colorbar\n",
    "        marker_line_width=0.8, marker_opacity=.5, colorscale=\"blues\", #adjust format of the plot\n",
    "        zmin=zmin, zmax=zmax,           #sets min and max of the colorbar\n",
    "        hovertemplate = \"<b>%{text}</b><br>\" +\n",
    "                    \n",
    "                    \"<extra></extra>\")  # sets the format of the text shown when you hover over each shape\n",
    "\n",
    "# Set the layout for the map\n",
    "layout = go.Layout(\n",
    "    title = {'text': f\"WFO's Messaging Flooding\",\n",
    "            'font': {'size':24}},       #format the plot title\n",
    "    mapbox1 = dict(\n",
    "        domain = {'x': [0, 1],'y': [0, 1]}, \n",
    "        center = dict(lat=37 , lon=-94),\n",
    "        accesstoken = MAPBOX_ACCESSTOKEN, \n",
    "        zoom = 3),                      \n",
    "    autosize=True,\n",
    "    height=650,\n",
    "    margin=dict(l=0, r=0, t=0, b=0))\n",
    "\n",
    "# Generate the map\n",
    "fig=go.Figure(data=data, layout=layout)\n",
    "fig.write_html('WFOmessages.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup_original_1 = BeautifulSoup(open('WFOmessages.html'))\n",
    "soup_original_2 = BeautifulSoup(open('wfo_messages.html'))\n",
    "for element in soup_original_2.body:\n",
    "    soup_original_1.body.append(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Locollaly\n",
    "with open(\"WFO_messaging_app.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(str(soup_original_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For saving on WPOD Drive\n",
    "#with open(\"//nwcal-fs2/WPOD/research_projects/WFO_AFD_Scanner/WFO_messaging_app.html\", \"w\", encoding='utf-8') as file:\n",
    "#    file.write(str(soup_original_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Open HTML table\n",
    "webbrowser.open('file://' + os.path.realpath('WFO_messaging_app.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
